{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, MaxPooling2D, Conv1D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "sns.set(style='white', context='notebook', palette='deep')\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# datetime object containing current date and time\n",
    "start_time = datetime.now()\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movie lens 100 K dataset\n",
    "import os\n",
    "# data_dir=\"C:/Users/ebrah/Desktop/Ryerson/research1/database/movielens_100K/ml-100k/ml-100k\"\n",
    "names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "# movie_db = pd.read_csv(os.path.join(data_dir, 'u.data'), '\\t', names=names,\n",
    "#                        engine='python')\n",
    "movie_db = pd.read_csv('C:/Users/ebrah/Desktop/Ryerson/research1/source_db/netflix_sample_complete.csv',skiprows=1, names = ['user_id','rating','date','item_id'],header = None)\n",
    "\n",
    "movie_db.head()\n",
    "# movie_db['date'] = pd.to_datetime(movie_db['timestamp'].astype(int), unit='s')\n",
    "# movie_db['date'] = pd.to_datetime(movie_db['date'],format=\"%m/%d/%Y\").dt.strftime('%Y-%m-%d')\n",
    "movie_db=movie_db[['user_id','item_id','rating','date']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#movie_db['date'].nunique() # 2000 user, 13344 item , 1951 date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(Y_train, pred_train):\n",
    "    true_positives = K.sum(K.round(K.clip(Y_train * pred_train, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(Y_train, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(Y_train, pred_train):\n",
    "    true_positives = K.sum(K.round(K.clip(Y_train * pred_train, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(pred_train, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(Y_train, pred_train):\n",
    "    precision = precision_m(Y_train, pred_train)\n",
    "    recall = recall_m(Y_train, pred_train)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n",
    "#################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack day is:  ['2003-05-31']\n"
     ]
    }
   ],
   "source": [
    "item_info=movie_db.groupby('item_id', as_index=False).rating.agg({np.mean, np.var})\n",
    "item_info.reset_index(level=0, inplace=True)\n",
    "target_size=100\n",
    "selected_item_size=20\n",
    "performance=pd.DataFrame(columns=['attack_model','attack size','filler size','test_or_train','accuracy','recall','F1'])\n",
    "\n",
    "x=0.01 ## percentage of fillers for AOP attack\n",
    "avg_rating=movie_db['rating'].mean() \n",
    "std_rating=movie_db['rating'].std() \n",
    "#selected_items to be chosen from highly popular items , items that are rated by most of users ( highest number of ratings)\n",
    "#item_info1=\n",
    "#movie_db.groupby('item_id', as_index=False)['rating'].mean()\n",
    "attack_date=random.sample(list(movie_db['date'].unique()),1)\n",
    "print('attack day is: ',attack_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of fake profiles generated is: \n",
      " 200\n",
      "number of target items selected is: \n",
      " 100\n",
      "number of selected items  is: \n",
      " 2\n",
      "number of filler items selected for AOP attack is: \n",
      " 133\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-677cde928584>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[0miix_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMultiIndex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_product\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmovie_db\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmovie_db\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m arr = (movie_db.pivot_table('rating', ['user_id', 'date'], 'item_id', aggfunc='sum')\n\u001b[0m\u001b[0;32m     92\u001b[0m          \u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miix_n\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m          .reshape(movie_db.user_id.nunique(),movie_db.date.nunique(),-1))\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mpivot_table\u001b[1;34m(self, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed)\u001b[0m\n\u001b[0;32m   6072\u001b[0m             \u001b[0mdropna\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6073\u001b[0m             \u001b[0mmargins_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmargins_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6074\u001b[1;33m             \u001b[0mobserved\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobserved\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6075\u001b[0m         )\n\u001b[0;32m   6076\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\pivot.py\u001b[0m in \u001b[0;36mpivot_table\u001b[1;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m         \u001b[0mtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfill_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36msort_index\u001b[1;34m(self, axis, level, ascending, inplace, kind, na_position, sort_remaining, by)\u001b[0m\n\u001b[0;32m   5077\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5078\u001b[0m         \u001b[0mbaxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5079\u001b[1;33m         \u001b[0mnew_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbaxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5080\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5081\u001b[0m         \u001b[1;31m# reconstruct axis if needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[0;32m   1395\u001b[0m         \u001b[0mnew_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1396\u001b[0m         return self.reindex_indexer(\n\u001b[1;32m-> 1397\u001b[1;33m             \u001b[0mnew_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1398\u001b[0m         )\n\u001b[0;32m   1399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy)\u001b[0m\n\u001b[0;32m   1255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1257\u001b[1;33m             \u001b[0mnew_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slice_take_blocks_ax0\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_tuple\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1258\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m             new_blocks = [\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_slice_take_blocks_ax0\u001b[1;34m(self, slice_or_indexer, fill_tuple)\u001b[0m\n\u001b[0;32m   1355\u001b[0m                             \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1356\u001b[0m                             \u001b[0mnew_mgr_locs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1357\u001b[1;33m                             \u001b[0mfill_tuple\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1358\u001b[0m                         )\n\u001b[0;32m   1359\u001b[0m                     )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mtake_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_tuple)\u001b[0m\n\u001b[0;32m   1312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1313\u001b[0m         new_values = algos.take_nd(\n\u001b[1;32m-> 1314\u001b[1;33m             \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1315\u001b[0m         )\n\u001b[0;32m   1316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\algorithms.py\u001b[0m in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, out, fill_value, mask_info, allow_fill)\u001b[0m\n\u001b[0;32m   1714\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"F\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1715\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1716\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1717\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1718\u001b[0m     func = _get_take_nd_function(\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#for attack_i in ('random', 'average', 'bandwagon','AOP'):\n",
    "    #for filler_i in (0.05):#(0.01,0.03,0.05,0.07,0.09):\n",
    "     #   \n",
    "#for attacksize_j in (0.1,0.15,0.2):\n",
    "attack_model='AOP'#attack_i\n",
    "filler_size= 0.01#filler_i # from 1% to 10% which is 1% of items excluding target items\n",
    "attack_size= 0.1\n",
    "fake_db = pd.DataFrame( columns=['user_id', 'item_id', 'rating', 'date'])\n",
    "fake_profile=[]\n",
    "filler_db=pd.DataFrame(columns=['user_id', 'item_id', 'rating', 'date'])\n",
    "target_db=pd.DataFrame(columns=['user_id', 'item_id', 'rating', 'date'])\n",
    "selected_db=pd.DataFrame(columns=['user_id', 'item_id', 'rating', 'date'])\n",
    "# defining fake profiles\n",
    "for i in range (round(attack_size*movie_db['user_id'].nunique())):\n",
    "    fake_profile.append(movie_db['user_id'].max()+(i+1))\n",
    "print('number of fake profiles generated is: \\n',round(attack_size*movie_db['user_id'].nunique()))\n",
    "\n",
    "#defining target items\n",
    "#30 items are choosen randomly out of 1682 items\n",
    "target_items=random.sample(list(movie_db['item_id'].unique()),target_size)\n",
    "print('number of target items selected is: \\n',target_size)\n",
    "\n",
    "\n",
    "#defining filler items\n",
    "fillers = [x for x in list(movie_db['item_id']) if x not in target_items]\n",
    "#randomly choosed filler items based on filler size\n",
    "filler_items=random.sample(fillers,round(filler_size*len(set(fillers))))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#defining selected items\n",
    "selected=movie_db.groupby('item_id', as_index=False)['rating'].count()\n",
    "selected=selected[selected['rating']>round(0.4*movie_db['user_id'].nunique())]\n",
    "selected_items=[x for x in selected['item_id'] if x not in target_items if x not in filler_items]\n",
    "print('number of selected items  is: \\n',len(set(selected_items)))\n",
    "\n",
    "#insert ratings for filler items\n",
    "filler_db.drop(filler_db.index, inplace=True)\n",
    "for i in filler_items:\n",
    "    if attack_model=='random':\n",
    "        filler_db=filler_db.append(pd.DataFrame({'user_id': random.sample(fake_profile,1), 'item_id': i ,'rating':round(np.random.normal(avg_rating, std_rating),2),'date':[attack_date[0]]}), ignore_index=True)\n",
    "    if attack_model=='average':\n",
    "        filler_db=filler_db.append(pd.DataFrame({'user_id': random.sample(fake_profile,1), 'item_id': i ,'rating':item_info.loc[item_info['item_id']==i,'mean'],'date':[attack_date[0]]}), ignore_index=True)\n",
    "    if attack_model=='bandwagon':\n",
    "        filler_db=filler_db.append(pd.DataFrame({'user_id': random.sample(fake_profile,1), 'item_id': i ,'rating':round(np.random.normal(avg_rating, std_rating),2),'date':[attack_date[0]]}), ignore_index=True)\n",
    "\n",
    "\n",
    "#insert ratings for target items\n",
    "target_db.drop(target_db.index, inplace=True)\n",
    "for i in target_items:\n",
    "    #print(random.sample(fake_profile,1),i,5)\n",
    "    target_db=target_db.append(pd.DataFrame({'user_id':random.sample(fake_profile,1),'item_id':i,'rating':5,'date':[attack_date[0]]}),ignore_index=True)\n",
    "\n",
    "#insert rating for selected items\n",
    "if attack_model=='bandwagon':\n",
    "    for i in selected_items:\n",
    "        selected_db=selected_db.append(pd.DataFrame({'user_id': random.sample(fake_profile,1), 'item_id': i ,'rating':5,'date':[attack_date[0]]}), ignore_index=True)\n",
    "\n",
    "\n",
    "#fillers selection for AOP attacks\n",
    "\n",
    "AOP_filler_db=pd.DataFrame(columns=['user_id', 'item_id', 'rating', 'date'])\n",
    "AOP_fillers=movie_db.groupby('item_id', as_index=False)['rating'].count()\n",
    "AOP_fillers=AOP_fillers.sort_values('rating',ascending=False)\n",
    "AOP_fillers=AOP_fillers.head(int(len(AOP_fillers)*(x)))\n",
    "AOP_fillers=[x for x in AOP_fillers['item_id']]\n",
    "\n",
    "\n",
    "if attack_model=='AOP':\n",
    "    for i in AOP_fillers:\n",
    "        AOP_filler_db=AOP_filler_db.append(pd.DataFrame({'user_id':random.sample(fake_profile,1), 'item_id': i ,'rating':np.random.normal(item_info.loc[item_info['item_id']==i,'mean'],item_info.loc[item_info['item_id']==i,'var']),'date':[attack_date[0]]}), ignore_index=True)\n",
    "    filler_db=AOP_filler_db\n",
    "\n",
    "if attack_model=='AOP':\n",
    "    print('number of filler items selected for AOP attack is: \\n',round(len(set(AOP_fillers))))\n",
    "else:\n",
    "    print('number of filler items selected is: \\n',round(filler_size*len(set(fillers))))\n",
    "\n",
    "# to first create a complete dataframe and then reshape to array\n",
    "injected_db=pd.DataFrame()\n",
    "injected_db=injected_db.append(target_db)\n",
    "injected_db=injected_db.append(filler_db)\n",
    "if attack_model=='bandwagon':\n",
    "    injected_db=injected_db.append(selected_db)\n",
    "\n",
    "movie_db=movie_db.append(injected_db,sort=True)\n",
    "movie_db.fillna(0, inplace=True)\n",
    "\n",
    "iix_n = pd.MultiIndex.from_product([np.unique(movie_db.user_id), np.unique(movie_db.date)])\n",
    "arr = (movie_db.pivot_table('rating', ['user_id', 'date'], 'item_id', aggfunc='sum')\n",
    "         .reindex(iix_n,copy=False).to_numpy()\n",
    "         .reshape(movie_db.user_id.nunique(),movie_db.date.nunique(),-1))\n",
    "arr_y=[1  for i in arr[:2000,:,:]]\n",
    "arr_y2=[0  for i in arr[2000:,:,:] ]########??????????? how to give target to them????\n",
    "arr_y3=arr_y+arr_y2\n",
    "\n",
    "inds = np.where(np.isnan(arr))\n",
    "#Place column means in the indices. Align the arrays using take\n",
    "arr[inds] = 0\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(arr,arr_y3,test_size=0.30, random_state=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "del(arr)\n",
    "del(inds)\n",
    "\n",
    "##model creation\n",
    "n_users=X_train.shape[0]\n",
    "n_days=X_train.shape[1]\n",
    "n_items=X_train.shape[2]\n",
    "#input_shape=(1,732,213,1682)\n",
    "#input_shape=(213,)\n",
    "timesteps = 1\n",
    "data_dim=1951\n",
    "kernel_size=4\n",
    "filters=32\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "################\n",
    "#(batch_size, channels, rows, cols)\n",
    "#, data_format=\"channels_last\"\n",
    "# ba in kar mikone : 1,n_users,n_days, n_items\n",
    "#va in 1,n_days,n_users,n_items\n",
    "model.add(TimeDistributed(Conv2D(32, (3, 3), activation='relu'), input_shape =(1,n_days,n_items,1)))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(3,3))))\n",
    "model.add(TimeDistributed(Dropout(0.5)))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(LSTM(32\n",
    "               , return_sequences=False,\n",
    "               input_shape=(timesteps, data_dim)\n",
    "              )\n",
    "         )\n",
    "## change data shape, to one 2D for every day and time steps as number of days\n",
    "####model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "\n",
    "model.summary()\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy',f1_m,precision_m, recall_m]) \n",
    "Y_train = to_categorical(Y_train) ############\n",
    "Y_test = to_categorical(Y_test)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0],1,X_train.shape[1],  X_train.shape[2],1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0],1,X_test.shape[1],  X_test.shape[2],1))\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=5,batch_size=20)\n",
    "\n",
    "pred_train= model.predict(X_train, batch_size=128)\n",
    "#scores = model.evaluate(X_train, Y_train, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(X_train, Y_train, verbose=0)\n",
    "print('Accuracy on training data: {}% \\n Error on training data: {}'.format(accuracy, 1 - accuracy))   \n",
    "print('Recall on training data: {}% \\n Error on training data: {}'.format(recall, 1 - recall)) \n",
    "print('F1 score on training data: {}% \\n Error on training data: {}'.format(f1_score, 1 - f1_score)) \n",
    "\n",
    "pred_test= model.predict(X_test)\n",
    "loss2, accuracy2, f1_score2, precision2, recall2  = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Accuracy on test data: {}% \\n Error on test data: {}'.format(accuracy2, 1 - accuracy2))\n",
    "print('Recall on test data: {}% \\n Error on test data: {}'.format(recall2, 1 - recall2)) \n",
    "print('F1 score on test data: {}% \\n Error on test data: {}'.format(f1_score2, 1 - f1_score2)) \n",
    "pred_train2=np.argmax(pred_train, axis=1)\n",
    "#pred_train2 = np.round(pred_train)\n",
    "\n",
    "Y_train2=np.argmax(Y_train, axis=1)\n",
    "#Y_train2 = np.round(Y_train)\n",
    "\n",
    "pred_test2=np.argmax(pred_test, axis=1)\n",
    "#pred_test2 = np.round(pred_test)\n",
    "\n",
    "Y_test2=np.argmax(Y_test, axis=1)\n",
    "#Y_test2 = np.round(Y_test)\n",
    "performance=performance.append(pd.DataFrame({'attack_model': [attack_model],'attack size':[attack_size],'filler size':[filler_size], 'test_or_train': 'train' ,'accuracy':[accuracy],'recall':[recall], 'F1':[f1_score]}), ignore_index=True)\n",
    "performance=performance.append(pd.DataFrame({'attack_model': [attack_model],'attack size':[attack_size],'filler size':[filler_size], 'test_or_train': 'test' ,'accuracy':[accuracy2],'recall':[recall2], 'F1':[f1_score2]}), ignore_index=True)\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performance.to_csv(\"C:/Users/ebrah/Desktop/Ryerson/research1/performance/hybrid/inputmethod1.csv\",mode='a',sep=',')\n",
    "end_time = datetime.now()\n",
    "print(\"started at: \",start_time)\n",
    "print(\"ended at: \",end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
